{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.context.SQLContext object at 0x7efd9bb916d8>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "import json\n",
    "from pyspark.ml.classification import LinearSVCModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SQLContext\n",
    "from json import loads, dumps\n",
    "from kafka import KafkaProducer\n",
    "import uuid\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Set the classpath to include the JAR file\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /home/jovyan/spark-streaming-kafka-0-8-assembly_2.11-2.0.0-preview.jar pyspark-shell'\n",
    "\n",
    "# Initialize a SparkContext\n",
    "sc = SparkContext(appName=\"Kafka Message Processing\")\n",
    "\n",
    "# Initialize a StreamingContext with a batch interval of 15 seconds\n",
    "ssc = StreamingContext(sc, 15)\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Kafka Message Processing\").config(\"spark.driver.allowMultipleContexts\", \"true\").config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\").config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/hive\").enableHiveSupport().getOrCreate()\n",
    "sqlContext = SQLContext(spark)\n",
    "# Tạo bảng Hive nếu chưa tồn tại\n",
    "sqlContext.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS WeatherData (\n",
    "        time int,\n",
    "        day int,\n",
    "        month int,\n",
    "        temperature DOUBLE,\n",
    "        feelslike DOUBLE,\n",
    "        wind DOUBLE,\n",
    "        cloud DOUBLE,\n",
    "        rain DOUBLE,\n",
    "        pressure DOUBLE,\n",
    "        weather STRING\n",
    "    )\n",
    "    USING HIVE\n",
    "\"\"\")\n",
    "# Define your Kafka parameters\n",
    "topics = [\"WeatherData\"]\n",
    "kafka_params = {\n",
    "    \"bootstrap.servers\": \"kafka:9093\"\n",
    "}\n",
    "\n",
    "# Create a direct stream from Kafka\n",
    "direct_stream = KafkaUtils.createDirectStream(ssc, topics, kafka_params)\n",
    "\n",
    "def publish_message(kafka_producer, topic_name, key, value):\n",
    "    try:\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        kafka_producer.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        kafka_producer.flush()\n",
    "        print('Message published successfully.')\n",
    "    except Exception as ex:\n",
    "        print(str(ex))\n",
    "\n",
    "# Define your data processing logic\n",
    "def process_data(rdd):\n",
    "    kafka_producer = KafkaProducer(bootstrap_servers=['kafka:9093'], api_version=(0, 10))\n",
    "\n",
    "    # Process each batch of RDDs\n",
    "    for message in rdd.collect():\n",
    "        # Process the Kafka message\n",
    "        loaded_model = LinearSVCModel.load(\"/home/jovyan/model\")\n",
    "        data = json.loads(message[1])\n",
    "        print(data)\n",
    "\n",
    "        # Extract the relevant fields from the JSON message\n",
    "        temperature = data[\"temperature\"]\n",
    "        feelslike = data[\"feelslike\"]\n",
    "        wind = data[\"wind\"]\n",
    "        cloud = data[\"cloud\"]\n",
    "        humidity = data[\"humidity\"]\n",
    "        time = data[\"time\"]\n",
    "        pressure = data[\"pressure\"]\n",
    "        month = data[\"month\"]\n",
    "\n",
    "        print(time)\n",
    "        print(month)\n",
    "\n",
    "        # Create a Spark DataFrame with matching column names\n",
    "        new_data = spark.createDataFrame([(temperature, feelslike, wind, cloud,pressure,time,month)],\n",
    "                                  [\"temperature\", \"feelslike\", \"wind\", \"cloud\", \"pressure\",\"time\",\"month\"])\n",
    "        new_data.show()\n",
    "        # Define feature columns\n",
    "        feature_columns = ['temperature', 'feelslike', 'wind', 'cloud', \"pressure\",\"time\",\"month\"]\n",
    "\n",
    "        # Combine features into a single feature vector\n",
    "        assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "        new_data = assembler.transform(new_data)\n",
    "\n",
    "        # Make predictions for the new data\n",
    "        predictions_new_data = loaded_model.transform(new_data)\n",
    "\n",
    "        # Show the prediction results for the new data\n",
    "        predictions_new_data.show()   \n",
    "        rs = predictions_new_data.withColumn(\"prediction\", col(\"prediction\").cast(IntegerType()))\n",
    "        rs = predictions_new_data.select(\"prediction\")\n",
    "\n",
    "        # Convert DataFrame `rs` to JSON\n",
    "        json_records = rs.toJSON().collect()\n",
    "        for json_record in json_records:\n",
    "            key = str(uuid.uuid4())\n",
    "            prediction = rs.iloc[0]['prediction']\n",
    "\n",
    "            data = {\n",
    "            'prediction': prediction\n",
    "            \n",
    "             }\n",
    "            value = json.dumps(data)\n",
    "            key_bytes = bytes(key, encoding='utf-8')\n",
    "            value_bytes = bytes(value, encoding='utf-8')\n",
    "            kafka_producer.send('WeatherPredictData', key=key_bytes, value=value_bytes)\n",
    "            kafka_producer.flush()\n",
    "            print('Message published successfully.')\n",
    "\n",
    "    if kafka_producer is not None:\n",
    "        kafka_producer.close()\n",
    "        \n",
    "\n",
    "# Apply your processing logic to each batch of RDDs\n",
    "direct_stream.foreachRDD(process_data)\n",
    "\n",
    "# Start the Spark Streaming context\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the context to terminate (Ctrl+C to stop)\n",
    "ssc.awaitTermination()\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LinearSVCModel\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import uuid\n",
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "import json\n",
    "from pyspark.ml.classification import LinearSVCModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SQLContext\n",
    "from json import loads, dumps\n",
    "from kafka import KafkaProducer\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "# Set the classpath to include the JAR file\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /home/jovyan/spark-streaming-kafka-0-8-assembly_2.11-2.0.0-preview.jar pyspark-shell'\n",
    "\n",
    "# Initialize a SparkContext\n",
    "sc = SparkContext(appName=\"Kafka Message Processing\")\n",
    "\n",
    "# Initialize a StreamingContext with a batch interval of 15 seconds\n",
    "ssc = StreamingContext(sc, 15)\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Kafka Message Processing\").config(\"spark.driver.allowMultipleContexts\", \"true\").config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\").config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/hive\").enableHiveSupport().getOrCreate()\n",
    "sqlContext = SQLContext(spark)\n",
    "# Tạo bảng Hive nếu chưa tồn tại\n",
    "sqlContext.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS WeatherStreamingData (\n",
    "        time int,\n",
    "        date STRING,\n",
    "        month int,\n",
    "        temperature DOUBLE,\n",
    "        feelslike DOUBLE,\n",
    "        wind DOUBLE,\n",
    "        cloud DOUBLE,\n",
    "        pressure DOUBLE,\n",
    "        weather STRING\n",
    "    )\n",
    "    USING HIVE\n",
    "\"\"\")\n",
    "# Define your Kafka parameters\n",
    "topics = [\"WeatherData\"]\n",
    "kafka_params = {\n",
    "    \"bootstrap.servers\": \"kafka:9093\"\n",
    "}\n",
    "\n",
    "# Create a direct stream from Kafka\n",
    "direct_stream = KafkaUtils.createDirectStream(ssc, topics, kafka_params)\n",
    "\n",
    "# Initialize Kafka producer outside the process_data function\n",
    "kafka_producer = KafkaProducer(bootstrap_servers=['kafka:9093'], api_version=(0, 10))\n",
    "\n",
    "def process_data(rdd):\n",
    "    # Process each batch of RDDs\n",
    "    for message in rdd.collect():\n",
    "        # Process the Kafka message\n",
    "        loaded_model = LinearSVCModel.load(\"/home/jovyan/model\")\n",
    "        data = json.loads(message[1])\n",
    "\n",
    "        # Extract the relevant fields from the JSON message\n",
    "        temperature = data[\"temperature\"]\n",
    "        feelslike = data[\"feelslike\"]\n",
    "        wind = data[\"wind\"]\n",
    "        cloud = data[\"cloud\"]\n",
    "        humidity = data[\"humidity\"]\n",
    "        time = data[\"time\"]\n",
    "        pressure = data[\"pressure\"]\n",
    "        month = data[\"month\"]\n",
    "        weather = data[\"weather\"]\n",
    "\n",
    "        date = datetime.datetime.now()\n",
    "\n",
    "        # Create a Spark DataFrame with matching column names\n",
    "        hive_data = spark.createDataFrame([( time,date ,month,temperature, feelslike, wind, cloud, pressure,weather)],\n",
    "                                  [\"time\",\"date\", \"month\",\"temperature\", \"feelslike\", \"wind\", \"cloud\", \"pressure\",\"weather\"])\n",
    "        \n",
    "        hive_data.write.mode(\"append\").insertInto(\"WeatherStreamingData\")\n",
    "\n",
    "        \n",
    "        # Create a Spark DataFrame with matching column names\n",
    "        new_data = spark.createDataFrame([(temperature, feelslike, wind, cloud, pressure, time, month)],\n",
    "                                  [\"temperature\", \"feelslike\", \"wind\", \"cloud\", \"pressure\", \"time\", \"month\"])\n",
    "\n",
    "        # Define feature columns\n",
    "        feature_columns = ['temperature', 'feelslike', 'wind', 'cloud', \"pressure\", \"time\", \"month\"]\n",
    "\n",
    "        # Combine features into a single feature vector\n",
    "        assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "        new_data = assembler.transform(new_data)\n",
    "\n",
    "        # Make predictions for the new data\n",
    "        predictions_new_data = loaded_model.transform(new_data)\n",
    "        rs = predictions_new_data.select(col(\"prediction\").cast(IntegerType()).alias(\"prediction\"))\n",
    "        rs.show()\n",
    "        # Convert DataFrame `rs` to JSON\n",
    "        json_records = rs.toJSON().collect()\n",
    "        for json_record in json_records:\n",
    "            key = str(uuid.uuid4())\n",
    "            prediction = rs.collect()[0][\"prediction\"]\n",
    "\n",
    "            data = {'prediction': prediction}\n",
    "\n",
    "            value = json.dumps(data)\n",
    "            key_bytes = bytes(key, encoding='utf-8')\n",
    "            value_bytes = bytes(value, encoding='utf-8')\n",
    "            kafka_producer.send('WeatherPredictData', key=key_bytes, value=value_bytes)\n",
    "            kafka_producer.flush()\n",
    "            print('Message published successfully')\n",
    "\n",
    "# Apply your processing logic to each batch of RDDs using foreachRDD\n",
    "direct_stream.foreachRDD(process_data)\n",
    "\n",
    "# Start the Spark Streaming context\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the context to terminate (Ctrl+C to stop)\n",
    "ssc.awaitTermination()\n",
    "sc.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
