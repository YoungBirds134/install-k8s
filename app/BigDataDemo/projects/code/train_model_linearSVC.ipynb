{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, substring, rand\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVCModel\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+---------+----+-----+--------+-------------+-------+-----+\n",
      "|time|month|temperature|feelslike|wind|cloud|pressure|      weather|predict|label|\n",
      "+----+-----+-----------+---------+----+-----+--------+-------------+-------+-----+\n",
      "|   1|    1|       24.0|     26.0| 8.0|  4.0|  1012.0|        Clear| norain|    0|\n",
      "|   1|    1|       24.0|     27.0| 8.0|  4.0|  1011.0|        Clear| norain|    0|\n",
      "|   1|    1|       24.0|     26.0| 8.0|  7.0|  1012.0|        Clear| norain|    0|\n",
      "|   1|    1|       28.0|     31.0|11.0|  6.0|  1012.0|        Clear| norain|    0|\n",
      "|   1|    1|       32.0|     35.0|10.0| 53.0|  1010.0|Partly cloudy| norain|    0|\n",
      "|   1|    1|       32.0|     36.0| 1.0| 79.0|  1009.0|       Cloudy| norain|    0|\n",
      "|   1|    1|       27.0|     30.0| 5.0| 43.0|  1010.0|Partly cloudy| norain|    0|\n",
      "|   1|    1|       26.0|     28.0| 8.0| 29.0|  1011.0|Partly cloudy| norain|    0|\n",
      "|   1|    1|       25.0|     27.0|10.0| 12.0|  1011.0|        Clear| norain|    0|\n",
      "|   1|    1|       24.0|     26.0| 8.0| 17.0|  1010.0|        Clear| norain|    0|\n",
      "|   1|    1|       24.0|     26.0|10.0| 28.0|  1011.0|Partly cloudy| norain|    0|\n",
      "|   1|    1|       28.0|     31.0|12.0|  9.0|  1012.0|        Clear| norain|    0|\n",
      "|   1|    1|       32.0|     35.0|12.0| 55.0|  1010.0|Partly cloudy| norain|    0|\n",
      "|   1|    1|       32.0|     36.0| 9.0| 19.0|  1009.0|        Clear| norain|    0|\n",
      "|   1|    1|       27.0|     31.0| 6.0| 32.0|  1011.0|Partly cloudy| norain|    0|\n",
      "|   1|    1|       26.0|     28.0| 9.0| 21.0|  1012.0|        Clear| norain|    0|\n",
      "|   1|    1|       24.0|     26.0| 9.0| 32.0|  1011.0|Partly cloudy| norain|    0|\n",
      "|   1|    1|       23.0|     25.0| 8.0| 38.0|  1011.0|Partly cloudy| norain|    0|\n",
      "|   1|    1|       23.0|     25.0| 8.0| 32.0|  1011.0|Partly cloudy| norain|    0|\n",
      "|   1|    1|       28.0|     30.0|10.0| 12.0|  1011.0|        Clear| norain|    0|\n",
      "+----+-----+-----------+---------+----+-----+--------+-------------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+-----+-----------+---------+----+-----+--------+-------+-------+-----+--------------------+--------------------+----------+\n",
      "|time|month|temperature|feelslike|wind|cloud|pressure|weather|predict|label|            features|       rawPrediction|prediction|\n",
      "+----+-----+-----------+---------+----+-----+--------+-------+-------+-----+--------------------+--------------------+----------+\n",
      "|   1|    1|       21.0|     21.0|10.0| 16.0|  1016.0|  Clear| norain|    0|[21.0,21.0,10.0,1...|[3.45392538792917...|       0.0|\n",
      "|   1|    1|       21.0|     21.0|13.0|  4.0|  1015.0|  Clear| norain|    0|[21.0,21.0,13.0,4...|[4.31003870955442...|       0.0|\n",
      "|   1|    1|       21.0|     21.0|17.0|  6.0|  1015.0|  Clear| norain|    0|[21.0,21.0,17.0,6...|[4.14934944876494...|       0.0|\n",
      "|   1|    1|       22.0|     22.0| 9.0| 11.0|  1017.0|  Clear| norain|    0|[22.0,22.0,9.0,11...|[3.82199817714227...|       0.0|\n",
      "|   1|    1|       22.0|     22.0|10.0| 83.0|  1015.0| Cloudy| norain|    0|[22.0,22.0,10.0,8...|[-1.4247101280773...|       1.0|\n",
      "+----+-----+-----------+---------+----+-----+--------+-------+-------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEYCAYAAAByXKB5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGz1JREFUeJzt3X28ZWVd9/HPlwEEIRFkUJwBBwXlBkqUh7jNuDFR8RHKUHwCDKUUU28tA4saKXyldls3JRpqMaRCmE+kYuoQIqbiICAOgkwwAoI8CYiIJPDrj3VNbA77nLPPmtlzzoHP+/Xar732da1rrWvvM7O+e13rYaeqkCRppjaY7Q5IkuYnA0SS1IsBIknqxQCRJPVigEiSejFAJEm9GCBSk+SgJJ9KsmAdL/eUJK9dl8uU5gIDROtckrOTvGaaeV6f5OYkC8fYj6VJPjLNPPskuSXJHwFbAy+vqnuSfCDJsZO0OTzJudMs931Jrk/yDOCzVfXB3m9khpL8/yQ3JTlmhu2m/bxmsKxpP6MhbTZKcmGS562LPmj8DBD1kmR1kjuT/LRtKP8xyeYjtn0kcDjwu8Cfj7Of0/RjA+B4YG/gucDnq+pOgKr6varq1bckuwOL2zLfWFWnD9RVkh3XuvNTexfwCuC2Ma/nAZI8PcmtwInAPkmWzaD5MXRh+/nx9E7rmgGitfHCqtoceCqwF/AnI7bbCXhDVf0L8K0kG4+rg9MI8OKquhx4HnDLOlruKuAlVfVt4Ih1tMyRVdW1wIZVdeK6XnY6r5xi3edW1SOB1wPfqKrDRlzuAuBW4E/XTU+1PhggWmtV9UPgTGC3geLHJflaktuTfDHJ1gN1bwPOSHIbcBhdoACQ5OQ2/PO51vabSZ4wbL1JlrRv9EcmuTbJdUneOmG2jdsxiNuTrEyy50DdE4FPt2/M3wKeNaEffzHK+0/yniTnJtmiFb0EuDDJLcBpSZa0+c5p9Re1PbeXDlnWgiR/1YagrkhyVHuPG7b61Un2H5j/fsNOST6e5EfAx5Kck2TXKfq9Q5KvtM/mS3RDeGvq9ktyzYT5C/htur/ZxGU9KskZSX6S5DzgCRPqn5bkW0lua89PG6h7NfBdur3BVUl+d2I/krw1yQ3tb/zqyd6T1i8DRGstyXZ03+AvGCh+OfBqYBtgY+APBurOpAuNbYBvAx+dsMiXAe8AtqT7Nn/8NF14Rlves4GjBzewwIuA04BHAmcAf9f6vBHwr8AXWz9+H/hokidN+4abJBsk+SDwK8Czq+q2JAcBf0y3oV0IfA04PUmqat/W9MlVtXlV/fOQxb4WeAHwFGDPtpyZmO6zHfQx4Hy64PhzhgTDEKcDZw0pfx/wc2Bb4HfaA4AkWwGfA04AHgW8F/hckke1WW6ie8+PoPs389dJnjqw7McAWwCL6Pbo3pdkyxH6qnGrKh8+ZvwAVgM/pRt2+AHdmPemre5s4E8G5n098IVJlvNIoIAt2uuTgQ8N1D8PuHSStkta250Hyt4NfLhNLwW+PFC3C3Bnm/514EfABgP1pwJLB/rxF5Os93Dgm8A/A58ANh6oOxN47cDrBcCdwJL2uoAdp/hczwJ+b+D1s1ubDQc+9/0H6pcCHxnls51Qtz1wN7DZQNnH1iwL2A+4ZsjffP8hy1oA/GLC3+GdwLlt+lXAeRPafB04fJJ+fxp400A/7lzz/lvZDcA+s/1/wEexIVJ/B1XVlyep+9HA9M+AzeF/xrqPBw6m+4Z+b5tna+476Du07RSuHpj+AfDLU/RjkzYc9Fjg6qq6d0LbRdOsa40dgScDe1fVfw2UPw44dsJQ2k/ovkWvHmG5j+WB72ckI362g+u5parumLCu7UZd34CFwIZM3u/H8sD38T+fdZJnAscCj2993hq4eGDem6vq7oHXo/yb0HrgEJbWt5cDBwL70w1LLGnlWYtlDm70tgeuHaHNtcB27UyswbY/HHGd36MbbjlzwrDX1cAfV9XOA49HV9U3RlzudTzw/Qy6A3j4wOvHDEzP5LO9DtgyyWaTrOt+62nhNNkp1zfS7c1M1u9r6YKVCfU/bCdQfAb4f8DjqmoJsHySPmuOMUC0vv0ScBdwM90G6p3rYJnHJnl4O2D8arqhpel8k24j+bZ2/cF+wAvpjpeMpKpOBd4OfHngQP8HgLcn2Q0gyRZJDh5odj3dN+3JnA68McniNs5/9IT6C4FDWp8nHiMZ+bOtqh8AK4B3JNk4ydPp3v8a36fbW3t+O150LLDJJMu6B/gksLT9HXbh/sdTPg88McnLk2zYTh7YBfgs8DBgU7q/BUmey8DJDJrbDBCtb6fQDV/8ELgEGPWb+VS+QnewfTnwV1X1xekatGGnF9Fdq3ET3TGcQ6vq0pmsuKqWAccBZyVZUlWfottwn5rkJ3RnFz13oMlSYFmSW5O8ZMgiPwj8G3AR3UHwT06oP5buDKdb6E40+NhA3Uw/25cDvwr8GPiz1n7N+7qN7tjVh+j2IO7i/kNUE72BbljpR3THj/5xYFk30x0kfytduL0NeEFV3VRVtwNvpDv+dEvr0xnT9FtzRNpBKWneaafHXglsNGGM/EHjofAeNX+5ByJpUu1akVtnux+amzwLS9KkqupKutOBpQdwCEuS1ItDWJKkXh7UQ1hbb711LVmyZLa7IUnzyvnnn39TVU37UwsP6gBZsmQJK1asmO1uSNK8kmSkOyA4hCVJ6sUAkST1YoBIknoxQCRJvRggkqReDBBJUi8GiCSpFwNEktSLASJJ6uVBfSX62vjTP/0brrrKu1hrPLbf/pEcd9ybZ7sb0loxQCZx1VW3smTJ0tnuhh6kVq9eOttdkNaaQ1iSpF4MEElSLwaIJKkXA0SS1IsBIknqxQCRJPVigEiSejFAJEm9GCCSpF4MEElSLwaIJKkXA0SS1IsBIknqxQCRJPVigEiSejFAJEm9GCCSpF4MEElSLwaIJKkXA0SS1IsBIknqxQCRJPVigEiSejFAJEm9GCCSpF4MEElSLwaIJKmX9RIgSRYkuSDJZ9vrrZJ8Kcnl7XnLgXmPSbIqyWVJnjNQvkeSi1vdCUmyPvouSRpufe2BvAn43sDro4HlVbUTsLy9JskuwCHArsABwIlJFrQ27weOBHZqjwPWT9clScOMPUCSLAaeD3xooPhAYFmbXgYcNFB+WlXdVVVXAquAvZNsCzyiqr5eVQWcMtBGkjQL1sceyN8AbwPuHSh7dFVdB9Cet2nli4CrB+a7ppUtatMTyyVJs2SsAZLkBcANVXX+qE2GlNUU5cPWeWSSFUlW3HjjjSOuVpI0U+PeA/k14EVJVgOnAb+R5CPA9W1YivZ8Q5v/GmC7gfaLgWtb+eIh5Q9QVSdV1Z5VtefChQvX5XuRJA0Ya4BU1TFVtbiqltAdHD+rql4JnAEc1mY7DPhMmz4DOCTJw5LsQHew/Lw2zHV7kn3a2VeHDrSRJM2CDWdpvX8JnJ7kCOAq4GCAqlqZ5HTgEuBu4Kiquqe1eR1wMrApcGZ7SJJmyXoLkKo6Gzi7Td8MPHOS+Y4Hjh9SvgLYbXw9lCTNhFeiS5J6MUAkSb0YIJKkXgwQSVIvBogkqRcDRJLUiwEiSerFAJEk9WKASJJ6MUAkSb0YIJKkXgwQSVIvBogkqRcDRJLUiwEiSerFAJEk9WKASJJ6MUAkSb0YIJKkXgwQSVIvBogkqRcDRJLUiwEiSerFAJEk9WKASJJ6MUAkSb0YIJKkXgwQSVIvBogkqRcDRJLUiwEiSerFAJEk9WKASJJ6MUAkSb0YIJKkXsYaIEk2SXJekouSrEzyjla+VZIvJbm8PW850OaYJKuSXJbkOQPleyS5uNWdkCTj7LskaWrj3gO5C/iNqnoysDtwQJJ9gKOB5VW1E7C8vSbJLsAhwK7AAcCJSRa0Zb0fOBLYqT0OGHPfJUlTGGuAVOen7eVG7VHAgcCyVr4MOKhNHwicVlV3VdWVwCpg7yTbAo+oqq9XVQGnDLSRJM2CsR8DSbIgyYXADcCXquqbwKOr6jqA9rxNm30RcPVA82ta2aI2PbF82PqOTLIiyYobb7xx3b4ZSdL/GHuAVNU9VbU7sJhub2K3KWYfdlyjpigftr6TqmrPqtpz4cKFM++wJGkk6+0srKq6FTib7tjF9W1YivZ8Q5vtGmC7gWaLgWtb+eIh5ZKkWbLhKDMlOWFI8W3Aiqr6zBTtFgK/qKpbk2wK7A+8CzgDOAz4y/a8ZhlnAB9L8l7gsXQHy8+rqnuS3N4OwH8TOBT421H6Lkkaj5ECBNgE2Bn4eHv9YmAlcESSZ1TVmydpty2wrJ1JtQFwelV9NsnXgdOTHAFcBRwMUFUrk5wOXALcDRxVVfe0Zb0OOBnYFDizPSRJs2TUANmR7nTcuwGSvB/4IvAs4OLJGlXVd4CnDCm/GXjmJG2OB44fUr4CmOr4iSRpPRr1GMgiYLOB15sBj217B3et815Jkua8UfdA3g1cmORsujOi9gXemWQz4Mtj6pskaQ4bKUCq6sNJPg/sTRcgb6+qNWdB/eG4OidJmrtmchrvBsCNwI+BHZPsO54uSZLmg1FP430X8FK6M6/ubcUFnDOmfkmS5rhRj4EcBDypqjxgLkkCRh/CuoLuRoiSJAGj74H8jO4srOUMnLZbVW8cS68kSXPeqAFyRntIkgRMEyBJHg6cQHf/qWXAE1vVZVX1izH3TZI0h00ZIFX1M+A1Se4B/hj4Ad11INslOayqPAtLkh6iRh3C+jnwnKq6DCDJE4FTgT2S7FxVl46rg5KkuWnUs7D+c014AFTV97kvfN66znslSZrzRt0DWZHkw8A/tdevAL4NUFWvHUfHJElz26gB8jrgKOCNdMdAzgFOHFenJElz33RnYW1G96uBW1TVocB710uvJElz3nRnYd0B/H6Se5MsHlL/G2PrmSRpTht1CGuvgelN6H7S9r/WfXckSfPFqL8Hcv6Eoq8l+coY+iNJmidGvZ37VgMvNwD2AB4zlh5JkuaFUYewzqf7/Y8AdwNXAkeMq1OSpLlv1CGsHcbdEUnS/DLdaby/NVV9VX1y3XZHkjRfTLcH8sIp6gowQCTpIWq660Bevb46IkmaX0a9maIkSfdjgEiSepk0QJIsWJ8dkSTNL1MdA/l4ksOr6idJNqK7I+++re4rwAf8WVupnwsuuIjDD186293Qg9T22z+S445789jXM1WAvBXYN8nNdBcNbsR9t3B/FfB+4DXj7Z704HTHHcWSJUtnuxt6kFq9eul6Wc+kAVJVVyZ5H/DbwF5V9eSB6rOSXDT23kmS5qzpDqKvBJ4C3JPkCWsKkzweuGecHZMkzW3TXQfyh0k2Af4Q+PckV9DdD+txgNeISNJD2LT3wqqqnwPLk+wEPIkuQC6tqrvG3TlJ0tw15RBWkr2SPAagBcbuwHHAeybc4n2y9tsl+fck30uyMsmbWvlWSb6U5PL2vOVAm2OSrEpyWZLnDJTvkeTiVndCkvR8z5KkdWC6YyB/T/vlwST70v0++inAbcBJIyz/buCtVfW/gH2Ao5LsAhwNLK+qnYDl7TWt7hBgV+AA4MSB61HeDxwJ7NQeB4z4HiVJYzBdgCyoqh+36ZcCJ1XVJ6rqWGDH6RZeVddV1bfb9O3A94BFwIHAsjbbMuCgNn0gcFpV3VVVVwKrgL2TbAs8oqq+XlVFF2IHIUmaNdMGSJI1x0meCZw1UDfqj1EBkGQJ3Rld3wQeXVXXQRcywDZttkXA1QPNrmlli9r0xHJJ0iyZLgROBb6S5CbgTuCrAEl2pBvGGkmSzYFPAG9uV7ZPOuuQspqifNi6jqQb6mL77bcftYuSpBmacg+kqo6nuyL9ZODpbfhoTbvfH2UF7TYonwA+OvADVNe3YSna8w2t/Bpgu4Hmi4FrW/niIeXD+nxSVe1ZVXsuXLhwlC5KknqY9m68VfWNqvpUVd0xUPb9Ncc2ptLOlPow8L2qeu9A1RnAYW36MOAzA+WHJHlYkh3oDpaf14a5bk+yT1vmoQNtJEmzYEbHMXr4Nbr7Zl2c5MJW9na6s7lOT3IEcBVwMEBVrUxyOnAJ3RlcR1XVmiveX0e3J7QpcGZ7SJJmyVgDpKrOZfjxC+gOyg9rczxw/JDyFcBu6653kqS14Q9KSZJ6MUAkSb0YIJKkXgwQSVIvBogkqRcDRJLUiwEiSerFAJEk9WKASJJ6MUAkSb0YIJKkXgwQSVIvBogkqRcDRJLUiwEiSerFAJEk9WKASJJ6MUAkSb0YIJKkXgwQSVIvBogkqRcDRJLUiwEiSerFAJEk9WKASJJ6MUAkSb0YIJKkXgwQSVIvBogkqRcDRJLUiwEiSerFAJEk9WKASJJ6MUAkSb0YIJKkXsYaIEn+IckNSb47ULZVki8lubw9bzlQd0ySVUkuS/KcgfI9klzc6k5IknH2W5I0vXHvgZwMHDCh7GhgeVXtBCxvr0myC3AIsGtrc2KSBa3N+4EjgZ3aY+IyJUnr2VgDpKrOAX48ofhAYFmbXgYcNFB+WlXdVVVXAquAvZNsCzyiqr5eVQWcMtBGkjRLZuMYyKOr6jqA9rxNK18EXD0w3zWtbFGbnlg+VJIjk6xIsuLGG29cpx2XJN1nLh1EH3Zco6YoH6qqTqqqPatqz4ULF66zzkmS7m82AuT6NixFe76hlV8DbDcw32Lg2la+eEi5JGkWzUaAnAEc1qYPAz4zUH5Ikocl2YHuYPl5bZjr9iT7tLOvDh1oI0maJRuOc+FJTgX2A7ZOcg3wZ8BfAqcnOQK4CjgYoKpWJjkduAS4Gziqqu5pi3od3RldmwJntockaRaNNUCq6mWTVD1zkvmPB44fUr4C2G0ddk2StJbm0kF0SdI8YoBIknoxQCRJvRggkqReDBBJUi8GiCSpFwNEktSLASJJ6sUAkST1YoBIknoxQCRJvRggkqReDBBJUi8GiCSpFwNEktSLASJJ6sUAkST1YoBIknoxQCRJvRggkqReDBBJUi8GiCSpFwNEktSLASJJ6sUAkST1YoBIknoxQCRJvRggkqReDBBJUi8GiCSpFwNEktSLASJJ6sUAkST1YoBIknoxQCRJvcyrAElyQJLLkqxKcvRs90eSHsrmTYAkWQC8D3gusAvwsiS7zG6vJOmha94ECLA3sKqqrqiq/wJOAw6c5T5J0kNWqmq2+zCSJL8NHFBVr2mvXwX8alW9YcJ8RwJHtpdPAi7rucqtgZt6tpWk2bS226/HVdXC6WbacC1WsL5lSNkD0q+qTgJOWuuVJSuqas+1XY4krW/ra/s1n4awrgG2G3i9GLh2lvoiSQ958ylAvgXslGSHJBsDhwBnzHKfJOkha94MYVXV3UneAPwbsAD4h6paOcZVrvUwmCTNkvWy/Zo3B9ElSXPLfBrCkiTNIQ/6AEny0iS/Otv9kKR1oW3Tlsx2P2AeBUiSnw5MPy/J5Um2T3Jyu0ZkWJsDgEXA7yRZvL76Opkkm812HyTNXUkek+S0JP+Z5JIkn0/yxIH6VwLbV9XqdbzejZNsNNN28+Yg+hpJngn8LfDsqroqGXZ5SKeqvgB8YX31bQRvTvLhqvrRbHdE0tySbmP2KWBZVR3SynYHHg18H6CqPjKm1e9Fd5nEaTNpNG/2QACS/DrwQeD5VfWfA1X7JvmPJFes2RtJ5z1Jvpvk4iQvbeX7JTk7yb8kuTTJR9sfbs2ezaVJzk1yQpLPDunD4Uk+neRfk1yZ5A1J3pLkgiTfSLJVm+/sJHu26a2TFPA04JeSfDXJt9vjaWP90CTNF88AflFVH1hTUFUXVtVXJ9ueDUqypG2/PtTm+2iS/ZN8rY3Y7N3mW5rkDwbaFfAPwEVt23Z+kpXtrh5Tmk97IA8DPgPsV1WXTqjbFng6sDPdtSH/AvwWsDvwZLrL+r+V5Jw2/1OAXekuRPwa8GtJVgB/D+xbVVcmOXWKvuzWlrEJsAr4o6p6SpK/Bg4F/mZImx9U1fOTPBx4VlX9PMlOwKmAV7xL2g04f5K6oduzqrpuwnw7AgfT3c7pW8DL6baNLwLeDhw0ZNkrgRdU1eokv1NVP06yaVvHJ6rq5sk6PJ/2QH4B/AdwxJC6T1fVvVV1Cd3uHnQf2qlVdU9VXQ98hW43DeC8qrqmqu4FLgSW0IXPFVV1ZZtnqgD596q6vapuBG4D/rWVX9yWNZWNgA8muRj4ON2dhSVpKlNtzwZdWVUXt23bSmB5dddqjLJtAnhjkouAb9ANae001czzKUDuBV4C7JXk7RPq7hqYzoTnYQbnv4duT2yq+adqf+/A63u5b6/ubu77fDcZmP//AtfTfZPYE9h4BuuV9OC1EthjkrpRt08z3TZB2z4l2Q/YH/jfVfVk4ALuv+16gPkUIFTVz4AXAK9IMmxPZNA5wEuTLEiyENgXOG+K+S8FHj9wetwDxhhnaDX3/WN48UD5FsB17RvCq+iuqpeks4CHJXntmoIkeyX5P8x8ezaV1cBT2/Kfyn17JlsAt1TVz5LsDOwz3YLmVYAAVNWPgQOAP0ky1e+BfAr4DnAR3R/mbVOd/VRVdwKvB76Q5Fy6vYTb1qKrfwW8Lsl/AI8aKD8ROCzJN4AnAnesxTokPUi0oabfBJ7VTuNdCSylO1Y7o+3ZND4BbJXkQuD3gMtb+ReADZN8B/hzumGsKXkrkwFJNq+qn7azst4HXF5Vfz3b/ZKkuWje7YGM2WtbKq+k2537+1nujyTNWe6BSJJ6cQ9EktSLASJJ6sUAkea4durmm5LMpztH6CHAAJFGkOSeJBe2ewRd1O5/Nun/n7bRH3oRVpJPJVmV5LYkh46w+rcAP62qu/v2XxoHA0QazZ1VtXtV7Qo8C3ge8GdTzP8Y4M3DKqrqN4HXAF+tqlOmWmkLqR9V1Yf7dVsaHwNEmqGquoHuZnVvaHdJPTzJ362pT3I53Z0N7nchVpID1tztme7meGvKt2p3Qf1Ou6Pzr7TyvYFzgbeku9v0k1r54Uk+meQL7S6r7x77m5aGMECkHqrqCrr/P9sMqb4MeGFVnb2moA1nfRB4IfDrdHsoa7wDuKCqfoXujqlr9koupbs79FPaPO8caLM73e12fpnuFhfbrYO3Jc2IB+Wk/mZyA86d6e6UejlAko/Q7cVAd6fVFwNU1VlJHpVkC2Bzujs3L2rrGrwlzvKquq0t6xLgccDVa/NmpJlyD0TqIcnj6e7kfAOT3N10iMmu2h0WRAX8Bd1PBzwdeOWE5Q67o7S0Xhkg0gy1u6F+APi7dgO81cDuSTZoQ0l7D2l2KbBDkie01y8bqDsHeEVb9n7ATVX1E2BL4MY2z+Hr+G1Ia81vLdJoNm33SduIbo/jn4D3trqvAVfS/WjPdxnyq3LtFyiPBD6X5Ca6g+O7teqlwD+2u6D+DDislb+nlb+F7g6s0pzivbAkSb04hCWtI0mOS7L/bPdDWl/cA5Ek9eIeiCSpFwNEktSLASJJ6sUAkST1YoBIknoxQCRJvfw3XBUNw8tl29IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the classpath to include the JAR file\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /home/jovyan/spark-streaming-kafka-0-8-assembly_2.11-2.0.0-preview.jar pyspark-shell'\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Kafka Weather Training Model \").config(\"spark.driver.allowMultipleContexts\", \"true\").config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\").config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/hive\").enableHiveSupport().getOrCreate()\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "\n",
    "\n",
    "data = sqlContext.sql(\"\"\"\n",
    "    SELECT * FROM WeatherData \n",
    "\"\"\")\n",
    "\n",
    "data = data.drop(\"rain\")\n",
    "data = data.drop(\"day\")\n",
    "\n",
    "\n",
    "# Xếp loại các loại thời tiết thành 2 dạng là có mưa và không mưa\n",
    "list_norain = ['Clear', 'Cloudy', 'Mist', 'Sunny', 'Partly cloudy', 'Thundery outbreaks possible']\n",
    "list_rain = ['Light drizzle','Light rain','Light rain shower', 'Patchy light drizzle', 'Patchy light rain', \n",
    "             'Patchy light rain with thunder','Patchy rain possible','Heavy rain','Heavy rain at times',\n",
    "             'Moderate or heavy rain shower','Moderate rain', 'Moderate rain at times', 'Overcast','Torrential rain shower']\n",
    "\n",
    "conditions = [\n",
    "    data['weather'].isin(list_norain),  # Check if 'weather' is in list_norain\n",
    "    data['weather'].isin(list_rain),    # Check if 'weather' is in list_rain\n",
    "]\n",
    "\n",
    "# Define corresponding values for each condition\n",
    "values = ['norain', 'rain']\n",
    "\n",
    "# Use the 'when' and 'lit' functions to create a new column 'predict'\n",
    "data = data.withColumn('predict', when(conditions[0], values[0]).when(conditions[1], values[1]).otherwise('unknown'))\n",
    "\n",
    "# Transformation for 'time' column\n",
    "time_expr = when((col('time').cast(\"int\") >= 6) & (col('time').cast(\"int\") <= 15), \"sang\").otherwise(\"toi\")\n",
    "#sang: 0 toi: 1\n",
    "time_label = when(data['time'] == 'sang', 0).otherwise(1)\n",
    "\n",
    "data = data.withColumn('time', time_label)\n",
    "\n",
    "# Transformation for 'month' column\n",
    "month_expr = when(col('month').isin([5, 6, 7, 8, 9, 10, 11]), \"mua\").otherwise(\"kho\")\n",
    "#mua: 0 kho: 1\n",
    "month_label = when(data['month'] == 'mua', int(0)).otherwise(int(1))\n",
    "data = data.withColumn('month', month_label)\n",
    "\n",
    "# Tạo cột nhãn 'label' cho mô hình\n",
    "label_expr = when(data['predict'] == 'rain', int(1)).otherwise(int(0))\n",
    "data = data.withColumn('label', label_expr)\n",
    "\n",
    "data.show()\n",
    "# Split the data into training (90%) and testing (10%) sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=100000)\n",
    "\n",
    "# Define feature columns\n",
    "feature_columns = ['temperature', 'feelslike', 'wind', 'cloud', 'pressure','time','month']\n",
    "\n",
    "# Combine features into a single feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "train_data = assembler.transform(train_data)\n",
    "test_data = assembler.transform(test_data)\n",
    "\n",
    "# Tạo mô hình LinearSVC\n",
    "lsvc = LinearSVC(labelCol=\"label\", maxIter=50)\n",
    "\n",
    "# Fit mô hình vào dữ liệu huấn luyện\n",
    "lsvc_model = lsvc.fit(train_data)\n",
    "#Chay Lan Dau\n",
    "#lsvc_model.write().overwrite().save('/home/jovyan/model')\n",
    "#lsvc_model.save('/home/jovyan/model')\n",
    "\n",
    "#Chay Lan Sau\n",
    "#lsvc_model.write().overwrite().save('/home/jovyan/model')\n",
    "\n",
    "# Dự đoán trên dữ liệu kiểm tra\n",
    "predictions = lsvc_model.transform(test_data)\n",
    "\n",
    "# Hiển thị kết quả dự đoán\n",
    "predictions.show(5)\n",
    "\n",
    "\n",
    "# Lấy kết quả dự đoán từ DataFrame\n",
    "predictions = lsvc_model.transform(test_data)\n",
    "\n",
    "# Chọn cột prediction (kết quả dự đoán) từ DataFrame\n",
    "prediction_values = predictions.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Vẽ biểu đồ phân phối dự đoán\n",
    "plt.hist(prediction_values, bins=2, alpha=0.5, color='b', edgecolor='black')\n",
    "plt.xlabel(\"Dự đoán\")\n",
    "plt.ylabel(\"Số lượng\")\n",
    "plt.title(\"Phân phối kết quả dự đoán\")\n",
    "plt.xticks([0, 1], [\"Không mưa\", \"Có mưa\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8753403066341883\n",
      "+-----------+---------+----+-----+--------+----+-----+--------------------+--------------------+----------+\n",
      "|temperature|feelslike|wind|cloud|pressure|time|month|            features|       rawPrediction|prediction|\n",
      "+-----------+---------+----+-----+--------+----+-----+--------------------+--------------------+----------+\n",
      "|      29.97|    36.97|   0|   20|    1013|   1|    1|[29.97,36.97,0.0,...|[3.13780891693668...|       0.0|\n",
      "+-----------+---------+----+-----+--------+----+-----+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Đánh giá mô hình\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Sử dụng mô hình để dự đoán dữ liệu mới\n",
    "new_data = spark.createDataFrame([(29.97, 36.97, 0, 20, 1013,1,1)],\n",
    "                                  [\"temperature\", \"feelslike\", \"wind\", \"cloud\",  \"pressure\",\"time\",\"month\"])\n",
    "#Received message: {\"temperature\": 29.97, \"feelslike\": 36.97, \"wind\": 0, \"cloud\": 20, \"humidity\": 84, \"pressure\": 1013, \"time\": \"sang\", \"month\": \"mua\"}\n",
    "# Áp dụng các biến đổi cho dữ liệu mới\n",
    "#new_data = new_data.withColumn('time', time_expr)\n",
    "#new_data = new_data.withColumn('month', month_expr)\n",
    "new_data = assembler.transform(new_data)\n",
    "\n",
    "# Dự đoán thời tiết mới\n",
    "predictions_new_data = lsvc_model.transform(new_data)\n",
    "\n",
    "# Hiển thị kết quả dự đoán cho dữ liệu mới\n",
    "predictions_new_data.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dừng phiên Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_5e4bc7342683\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o552.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 5.0 failed 1 times, most recent failure: Lost task 2.0 in stage 5.0 (TID 10, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 5, y.size = 6\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LinearSVCModel$$anonfun$12.apply(LinearSVC.scala:320)\n\tat org.apache.spark.ml.classification.LinearSVCModel$$anonfun$12.apply(LinearSVC.scala:319)\n\tat org.apache.spark.ml.classification.LinearSVCModel.predictRaw(LinearSVC.scala:328)\n\tat org.apache.spark.ml.classification.LinearSVCModel.predictRaw(LinearSVC.scala:298)\n\tat org.apache.spark.ml.classification.ClassificationModel$$anonfun$1.apply(Classifier.scala:168)\n\tat org.apache.spark.ml.classification.ClassificationModel$$anonfun$1.apply(Classifier.scala:167)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 5, y.size = 6\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LinearSVCModel$$anonfun$12.apply(LinearSVC.scala:320)\n\tat org.apache.spark.ml.classification.LinearSVCModel$$anonfun$12.apply(LinearSVC.scala:319)\n\tat org.apache.spark.ml.classification.LinearSVCModel.predictRaw(LinearSVC.scala:328)\n\tat org.apache.spark.ml.classification.LinearSVCModel.predictRaw(LinearSVC.scala:298)\n\tat org.apache.spark.ml.classification.ClassificationModel$$anonfun$1.apply(Classifier.scala:168)\n\tat org.apache.spark.ml.classification.ClassificationModel$$anonfun$1.apply(Classifier.scala:167)\n\t... 22 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1342fdcf1eb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Show the prediction results for the new data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpredictions_new_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o552.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 5.0 failed 1 times, most recent failure: Lost task 2.0 in stage 5.0 (TID 10, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 5, y.size = 6\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LinearSVCModel$$anonfun$12.apply(LinearSVC.scala:320)\n\tat org.apache.spark.ml.classification.LinearSVCModel$$anonfun$12.apply(LinearSVC.scala:319)\n\tat org.apache.spark.ml.classification.LinearSVCModel.predictRaw(LinearSVC.scala:328)\n\tat org.apache.spark.ml.classification.LinearSVCModel.predictRaw(LinearSVC.scala:298)\n\tat org.apache.spark.ml.classification.ClassificationModel$$anonfun$1.apply(Classifier.scala:168)\n\tat org.apache.spark.ml.classification.ClassificationModel$$anonfun$1.apply(Classifier.scala:167)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 5, y.size = 6\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LinearSVCModel$$anonfun$12.apply(LinearSVC.scala:320)\n\tat org.apache.spark.ml.classification.LinearSVCModel$$anonfun$12.apply(LinearSVC.scala:319)\n\tat org.apache.spark.ml.classification.LinearSVCModel.predictRaw(LinearSVC.scala:328)\n\tat org.apache.spark.ml.classification.LinearSVCModel.predictRaw(LinearSVC.scala:298)\n\tat org.apache.spark.ml.classification.ClassificationModel$$anonfun$1.apply(Classifier.scala:168)\n\tat org.apache.spark.ml.classification.ClassificationModel$$anonfun$1.apply(Classifier.scala:167)\n\t... 22 more\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"SVMExample\").getOrCreate()\n",
    "\n",
    "loaded_model = LinearSVCModel.load(\"/home/jovyan/model\")\n",
    "print(loaded_model)\n",
    "new_data = spark.createDataFrame([(29.97, 36.97, 0, 20, 1013)],\n",
    "                                  [\"temperature\", \"feelslike\", \"wind\", \"cloud\", \"pressure\"])\n",
    "        # Define feature columns\n",
    "feature_columns = ['temperature', 'feelslike', 'wind', 'cloud', 'pressure']\n",
    "\n",
    "        # Combine features into a single feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "new_data = assembler.transform(new_data)\n",
    "\n",
    "        # Make predictions for the new data\n",
    "predictions_new_data = loaded_model.transform(new_data)\n",
    "\n",
    "        # Show the prediction results for the new data\n",
    "predictions_new_data.show()   \n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
